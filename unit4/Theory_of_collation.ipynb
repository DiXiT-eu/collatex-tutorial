{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory of collation\n",
    "## Gothenburg model\n",
    "\n",
    "The Gotenburg model emerged from a meeting of the developers of CollateX and Juxta in Gothenburg, Sweden in 2009 at a joint workshop of the EU-funded research projects COST Action 32 and Interedition. (http://wiki.tei-c.org/index.php/Textual_Variance) The model conceptualizes collation as a pipeline that involves the following stages, which may be modularized, so that one or another can be modified without requiring revision of the rest of the system. Those stages are:\n",
    "\n",
    "1. Tokenization\n",
    "1. Normalization/regularization\n",
    "1. Alignment\n",
    "1. Analysis\n",
    "1. Visualization/output\n",
    "\n",
    "The description of these five stages below presupposes that the texts to be collated have already been transcribed and digitized.\n",
    "\n",
    "[Acknowledgement: Several images in this tutorial are copied from http://wiki.tei-c.org/index.php/Textual_Variance. Some of the textual content of this page was contributed by Helena Bermúdez Sabel.]\n",
    "\n",
    "\n",
    "### 1. Tokenization\n",
    "*Tokenization* is the division of a continuous text into units to be aligned (called *tokens*). Most commonly, tokens are whitespace-delimited words, but tokenization can be performed at any level of granularity, e.g., “syllables, words, lines, phrases, verses, paragraphs, or text nodes” (http://wiki.tei-c.org/index.php/Textual_Variance). Challenges that arise during tokenization include the following:\n",
    "\n",
    "* **Ambiguity.** In texts written without spaces between works (*scriptio continua*), the division into words may be ambiguous, that is, it may be possible to divide the same continuous writing in two ways, either of which would be linguistically correct.\n",
    "* **Punctuation.** Punctuation is commonly tokenized by itself, so that, for example, “cat” and “cat,” (without and with a trailing comma) will be recognized as instances of the same word. The situation is less clear with non-final punctuation, though, such as hyphenated words.\n",
    "* **Contractions** like English “doesn’t” or “can’t” raise questions about whether they should be treated as one word or two for the purpose of collation.\n",
    "\n",
    "The preceding issues affect tokenization on an intellectual level in that they involve decisions by the researcher that would arise with or without a computational environment. Machine-assisted tokenization, though, raises additional challenges, including the following: \n",
    "\n",
    "* **Word-internal punctuation** means that it cannot safely be assumed that a word is a continuous sequence of alphabetic characters, and that a punctuation character indicates the beginning of a new token. In addition to the ambiguities involving hyphenation and English negative contractions, consider:\n",
    "    * **Lexical contraction.** The contraction of “Amsterdam” as “A’dam”, which is lexically specific.\n",
    "    * **Punctuation before bound morphemes.** The English “-’s” possessive is different from English negative contractions in “-n’t”. The “-n’t” portion might be understood as a variant spelling of “not” (insofar as “doesn’t” may be replaced by “does not”, etc., without violating English language norms), but the “-’s” possessive particle does not have a free-standing lexical counterpart in modern English.\n",
    "* **Superscription.** In some scribal practice, such as Church Slavonic manuscripts, it is common to write some letters as superscripts, and the base and superscript letters may belong to different words. For example, “ona že” (‘and she’) may be written as “ona<sup>ž</sup>”. The visual form that appears in the manuscript isn’t easily reproduced here in a web interface, but the superscript “ž” is not merely raised, but also centered over the “a” that ends the preceding word.\n",
    "* **Markup**, such as XML, may be intermingled with textual content, and XML element tags may surround an entire word or part of a word, or they may begin inside one word and end inside another, which poses special challenges for tokenizating in a way that does not contradict XML well-formedness. Even where well-formedness is not an issue, researchers may differ in their preferences for taking markup into account when collating a set of witnesses.\n",
    "### 2. Normalization/regularization\n",
    "\n",
    "Some degree of normalization is inevitable during transcription because the editor has to transform handwriting, which is analog, into a digital representation. Every instance of a handwritten letter is likely to differ at least minutely from every other instance of the same letter, and the editors will nonetheless transcribe them the same way if they do not consider the differences meaningful. In diplomatic editions editors aim for maximal fidelity to the original, but even there some degree of conflation of variants is inevitable. The normalization process in the Gothenburg model does not refer to normalization during transcription. Normalization in the Gothenburg model refers to situations where the editor has transcribed two different written forms differently because the difference is important in some way, but it is not important for alignment. For that reason, the normalization stage of the Gothenburg model refers to the process of telling the alignment stage, which follows, to treat some graphically different forms as if they were the same for the purpose of aligning them.\n",
    "\n",
    "One common guideline in digital editing is to ignore *non-substantive* variation when performing comparisons for the purpose of alignment, although the understanding of non-substantive is both project-specific and subjective. Many (not all) editors may choose to normalize away differences in punctuation and in upper vs. lower case. Some editors may further elect to normalize orthographic variation (the use of what humans would regard as different spellings, with different letters), variant letterforms (what humans would regard as the same letters, but with different shapes), abbreviation, superscription, etc. If an editor considers these types of variation completely unimportant, they might be normalized during transcription, but if the variants need to be available for the output but should not be allowed to influence the alignment, then can be normalized between the tokenization and the alignment stages of the Gothenburg model. This type of alignment creates a normalized *shadow copy* of the literal token, so that the normalized form can be used to determine what should be considered the same for alignment purposes, but the eventual output can nonetheless preserve the original spelling.\n",
    "\n",
    "The types of textual variation that occur during the copying of texts may be categorized as the insertion of new textual material, the deletion of textual material present in the source, the substitution of new text in place of old, and the transposition of old text to a new location. Transposition can be encoded as deletion of material from one location and its insertion in another, but if the editor believes that the two acts are related, that is, that the material has been moved and the deletion and insertion are not independent of each other, the eventual edition should record that fact.\n",
    "\n",
    "\n",
    "Substantive ~ non-substantive\n",
    "Substantive: equipollent, linguistic, scribal error \n",
    "Non-substantive: graphic\n",
    "Ignore non-substantive variation for comparison\n",
    "Punctuation\n",
    "Upper ~ lower case\n",
    "Orthographic variation\n",
    "Variant letterforms\n",
    "Abbreviation\n",
    "\n",
    "\n",
    "As an extreme example, a normalization routine might map numbers written in digits to numbers written in words, so that “3” would be treated as identical to “three” for alignment purposes, but the output for each witness could preserve the original spelling in digits or alphabetic characters. Less extremely, upper- and lowercase spellings might be treated as identical for alignment purposes.\n",
    "\n",
    "*Markup normalization* requires attention at both the tokenization and the normalization stage. The markup is part of the stream of characters, so when the text is divided into words or other tokens, the tokenizer must know what to do with any markup it encounters. For example, the characters “text” in an XML tag `&lt;text&gt;` should not be treated the same way as the word “text” in the content of an element. This means that either the markup must be treated as something other than textual content during tokenization or it must be handled specially during normalization.\n",
    "### 3. Alignment\n",
    "<img align=\"right\" src=\"Collation_Aligner.png\"/>blah blah blah\n",
    "### 4. Analysis\n",
    "blah blah blah\n",
    "### 5. Visualization\n",
    "blah blah blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
